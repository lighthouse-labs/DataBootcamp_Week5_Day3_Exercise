{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of sales\n",
    "\n",
    "### Problem Statement\n",
    "[The dataset](https://drive.google.com/file/d/1B07fvYosBNdIwlZxSmxDfeAf9KaygX89/view?usp=sharing) represents sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store are available. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "|Variable|Description|\n",
    "|: ------------- |:-------------|\n",
    "|Item_Identifier|Unique product ID|\n",
    "|Item_Weight|Weight of product|\n",
    "|Item_Fat_Content|Whether the product is low fat or not|\n",
    "|Item_Visibility|The % of total display area of all products in a store allocated to the particular product|\n",
    "|Item_Type|The category to which the product belongs|\n",
    "|Item_MRP|Maximum Retail Price (list price) of the product|\n",
    "|Outlet_Identifier|Unique store ID|\n",
    "|Outlet_Establishment_Year|The year in which store was established|\n",
    "|Outlet_Size|The size of the store in terms of ground area covered|\n",
    "|Outlet_Location_Type|The type of city in which the store is located|\n",
    "|Outlet_Type|Whether the outlet is just a grocery store or some sort of supermarket|\n",
    "|Item_Outlet_Sales|Sales of the product in the particulat store. This is the outcome variable to be predicted.|\n",
    "\n",
    "Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\n",
    "\n",
    "\n",
    "\n",
    "### Explore the problem in following stages:\n",
    "\n",
    "1. Hypothesis Generation – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2. Data Exploration – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3. Data Cleaning – imputing missing values in the data and checking for outliers\n",
    "4. Feature Engineering – modifying existing variables and creating new ones for analysis\n",
    "5. Model Building – making predictive models on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis Generation\n",
    "\n",
    "This is a very pivotal step in the process of analyzing data. This involves understanding the problem and making some hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analysis which we can potentially perform if data is available.\n",
    "\n",
    "#### Possible hypotheses\n",
    "##### Store Level Hypotheses:\n",
    "\n",
    "1. City type: Stores located in urban or Tier 1 cities should have higher sales because of the higher income levels of people there.\n",
    "2. Population Density: Stores located in densely populated areas should have higher sales because of more demand.\n",
    "3. Store Capacity: Stores which are very big in size should have higher sales as they act like one-stop-shops and people would prefer getting everything from one place\n",
    "4. Competitors: Stores having similar establishments nearby should have less sales because of more competition.\n",
    "5. Marketing: Stores which have a good marketing division should have higher sales as it will be able to attract customers through the right offers and advertising.\n",
    "6. Location: Stores located within popular marketplaces should have higher sales because of better access to customers.\n",
    "7. Customer Behavior: Stores keeping the right set of products to meet the local needs of customers will have higher sales.\n",
    "8. Ambiance: Stores which are well-maintained and managed by polite and humble people are expected to have higher footfall and thus higher sales.\n",
    "\n",
    "##### Product Level Hypotheses:\n",
    "\n",
    "1. Brand: Branded products should have higher sales because of higher trust in the customer.\n",
    "2. Packaging: Products with good packaging can attract customers and sell more.\n",
    "3. Utility: Daily use products should have a higher tendency to sell as compared to the specific use products.\n",
    "4. Display Area: Products which are given bigger shelves in the store are likely to catch attention first and sell more.\n",
    "5. Visibility in Store: The location of product in a store will impact sales. Ones which are right at entrance will catch the eye of customer first rather than the ones in back.\n",
    "6. Advertising: Better advertising of products in the store will should higher sales in most cases.\n",
    "7. Promotional Offers: Products accompanied with attractive offers and discounts will sell more.\n",
    "\n",
    "We can think further and create some of our own. But remember, the data might not be sufficient to test all of these, but forming them enables a better understanding of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Perform some basic data exploration here and come up with some inferences about the data. Try to figure out some irregularities and address them in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8523, 12)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Read files:\n",
    "data = pd.read_csv(\"<data>\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key challenges in any data set is missing values. Lets start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some observations:\n",
    "\n",
    "Do min, max values make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to nominal (categorical) variable, lets have a look at the number of unique values in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many products are there, how many stores, item types?\n",
    "\n",
    "Explore the numbers of each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the output:\n",
    "\n",
    "Tipp: look at the unique values of `Item_Fat_Content`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "This step typically involves imputing missing values and treating outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values\n",
    "\n",
    "Which variables have missing values? How can you fill those values? Think of some suitalbe strategy (zero filling, mean, median values. Maybe mean values per `Item_Type`?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are no more missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "1. Resolving the issues in the data to make it ready for the analysis.\n",
    "2. Create some new variables using the existing ones.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a broad category of Type of Item\n",
    "\n",
    "Item_Type variable has many categories which might prove to be very useful in analysis. Look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. Use the Item_Identifier variable to create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# data['Item_Type_Combined'] ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the years of operation of a store\n",
    "\n",
    "Make a new column depicting the years of operation of a store (i.e. how long the store exists). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify categories of Item_Fat_Content\n",
    "\n",
    "There are difference in representation in categories of Item_Fat_Content variable. This should be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some non-consumables as well and a fat-content should not be specified for them. Create a separate category for such kind of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical and One-Hot Coding of Categorical variables\n",
    "\n",
    "Since scikit-learn accepts only numerical variables, convert all categories of nominal variables into numeric types. Created a new variable ‘Outlet’ same as Outlet_Identifier and code that too. O\n",
    "\n",
    "Encode all categorical variables as numeric using `LabelEncoder` from sklearn’s preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot-Coding refers to creating dummy variables, one for each category of a categorical variable. For example, the Item_Fat_Content has 3 categories – ‘Low Fat’, ‘Regular’ and ‘Non-Edible’. One hot coding will remove this variable and generate 3 new variables. Each will have binary numbers – 0 (if the category is not present) and 1(if category is present). This can be done using `get_dummies` function of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables are now float and each category has a new variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row will have only one of the columns as 1 corresponding to the category in the original variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data\n",
    "you can save the processed data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Its time to start making predictive models.\n",
    "\n",
    "Make a baseline model. Baseline model is the one which requires no predictive model and its like an informed guess. For instance, predict the sales as the overall average sales or just zero.\n",
    "Making baseline models helps in setting a benchmark. If your predictive algorithm is below this, there is something going seriously wrong and you should check your data.\n",
    "\n",
    "Split your data in 80% train set and 20% test set.\n",
    "Train the model in the training set and score the model using different evaluation metrics on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Define target\n",
    "target = 'Item_Outlet_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Models\n",
    "\n",
    "Try different  ensemble models (Random Forest Regressor, Gradient Boosting, XGBoost)\n",
    "\n",
    "Calculate the mean squared error on the test set. Explore how different parameters of the model affect the results and the performance of the model\n",
    "\n",
    "- Use GridSearchCV to find optimal paramaters of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
